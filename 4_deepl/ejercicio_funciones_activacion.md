# Ejercicio: Funciones de Activación

## 🎯 Objetivo
Comparar el comportamiento de Sigmoid, Tanh y ReLU.

---

## 🧩 Instrucciones
1. Implementa las funciones:
   - Sigmoid: `1 / (1 + exp(-x))`
   - Tanh: `(exp(x)-exp(-x)) / (exp(x)+exp(-x))`
   - ReLU: `max(0,x)`
2. Grafícalas en **matplotlib** para `x ∈ [-5, 5]`.
3. Compara las derivadas.

---

## 🧠 Preguntas
- ¿Por qué ReLU suele funcionar mejor en redes profundas?
- ¿Qué problemas presentan Sigmoid y Tanh?

---
[Regresar a la página anterior](./DeepLearning.md)

[Regresar al menú principal](../README.md)
