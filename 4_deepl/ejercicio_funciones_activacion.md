# Ejercicio: Funciones de ActivaciÃ³n

## ğŸ¯ Objetivo
Comparar el comportamiento de Sigmoid, Tanh y ReLU.

---

## ğŸ§© Instrucciones
1. Implementa las funciones:
   - Sigmoid: `1 / (1 + exp(-x))`
   - Tanh: `(exp(x)-exp(-x)) / (exp(x)+exp(-x))`
   - ReLU: `max(0,x)`
2. GrafÃ­calas en **matplotlib** para `x âˆˆ [-5, 5]`.
3. Compara las derivadas.

---

## ğŸ§  Preguntas
- Â¿Por quÃ© ReLU suele funcionar mejor en redes profundas?
- Â¿QuÃ© problemas presentan Sigmoid y Tanh?

---
[Regresar a la pÃ¡gina anterior](./DeepLearning.md)

[Regresar al menÃº principal](../README.md)
