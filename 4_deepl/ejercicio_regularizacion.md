# Ejercicio: Regularización y Optimización

## 🎯 Objetivo
Explorar Dropout, BatchNorm y Early Stopping.

---

## 🧩 Instrucciones
1. Usa un MLP con 2 capas ocultas.
2. Entrena en MNIST con:
   - Sin regularización
   - Con **Dropout = 0.5**
   - Con **BatchNorm**
3. Grafica accuracy vs epochs.

---

## 🧠 Preguntas
- ¿Cómo influye Dropout en el sobreajuste?
- ¿Cuándo conviene usar Early Stopping?

---
[Regresar a la página anterior](./DeepLearning.md)

[Regresar al menú principal](../README.md)
