# Ejercicio: Regularizaci칩n y Optimizaci칩n

## 游꿢 Objetivo
Explorar Dropout, BatchNorm y Early Stopping.

---

## 游빌 Instrucciones
1. Usa un MLP con 2 capas ocultas.
2. Entrena en MNIST con:
   - Sin regularizaci칩n
   - Con **Dropout = 0.5**
   - Con **BatchNorm**
3. Grafica accuracy vs epochs.

---

## 游 Preguntas
- 쮺칩mo influye Dropout en el sobreajuste?
- 쮺u치ndo conviene usar Early Stopping?

---
[Regresar a la p치gina anterior](./DeepLearning.md)

[Regresar al men칰 principal](../README.md)
