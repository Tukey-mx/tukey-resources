# Deep Learning

---

✅ -> Añadido  
✏️ -> En progreso

---

## Índice

| Tema | Estado |
|------|--------|
| [Neurona de McCulloch-Pitts](#neurona-de-mcculloch-pitts) | ✅ |
| [Perceptrón](#perceptrón) | ✅ |
| [Redes Feedforward (MLP)](#redes-feedforward-mlp) | ✅ |
| [Backpropagation](#backpropagation) | ✅ |
| [Funciones de activación](#funciones-de-activación) | ✅ |
| [Redes Radiales (RBF)](#redes-radiales-rbf) | ✅ |
| [Redes de Hopfield](#redes-de-hopfield) | ✅ |
| [Regularización y optimización](#regularización-y-optimización) | ✅ |
| [Redes convolucionales (CNN)](#redes-convolucionales-cnn) | ✅ |
| [Redes recurrentes (RNN)](#redes-recurrentes-rnn) | ✅ |
| [LSTM y GRU](#lstm-y-gru) | ✅ |
| [Sequence to Sequence (Seq2Seq)](#seq2seq) | ✅ |
| [Autoencoders y modelos generativos](#autoencoders-y-modelos-generativos) | ✅ |
| [Mecanismos de atención](#mecanismos-de-atención) | ✅ |
| [Transformers](#transformers) | ✅ |

---

## Neurona de McCulloch-Pitts
**Descripción:**  
La primera neurona artificial (1943), un modelo lógico binario que inspiró el perceptrón.

**Recursos:**  
| 🎥 Videos | 📝 Artículos | 📘 Libros | 🧠 Ejercicios |
|----------|--------------|-----------|---------------|
|  | [The Mind Project](https://mind.ilstu.edu/curriculum/mcp_neurons/index.html) | *Neural Networks and Learning Machines* (Haykin) | [Ejercicio](ejercicio_mcculloch_pitts.md) |

---

## Perceptrón
**Descripción:**  
Introducido por Frank Rosenblatt en 1958, clasificador lineal base de las redes neuronales.

**Recursos:**  
| 🎥 Videos | 📝 Artículos | 📘 Libros | 🧠 Ejercicios |
|----------|--------------|-----------|---------------|
| [video](https://www.youtube.com/watch?v=ntKn5TPHHAk) | [Matt Might – Hello, Perceptron](https://matt.might.net/articles/hello-perceptron/) | *Deep Learning* (Goodfellow) | [Ejercicio](ejercicio_perceptron.md) |

---

## Redes Feedforward (MLP)
**Descripción:**  
Redes multicapa donde la información fluye hacia adelante.

**Recursos:**  
| 🎥 Videos | 📝 Artículos | 📘 Libros | 🧠 Ejercicios |
|----------|--------------|-----------|---------------|
| [video](https://www.youtube.com/watch?v=aircAruvnKk) | [DeepLearningBook – Feedforward](https://www.deeplearningbook.org/contents/mlp.html) | *Deep Learning with Python* (Chollet) | [Ejercicio](ejercicio_feedforward_mlp.md) |

---

## Backpropagation
**Descripción:**  
Algoritmo de retropropagación que permite entrenar redes profundas ajustando pesos.

**Recursos:**  
| 🎥 Videos | 📝 Artículos | 📘 Libros | 🧠 Ejercicios |
|----------|--------------|-----------|---------------|
| [video](https://www.youtube.com/watch?v=tIeHLnjs5U8) | [Neptune.ai – Backpropagation](https://neptune.ai/blog/backpropagation-algorithm-in-neural-networks-guide) | *Neural Networks and Deep Learning* (Nielsen) | [Ejercicio](ejercicio_backpropagation.md) |

---

## Funciones de activación
**Descripción:**  
Sigmoid, Tanh, ReLU, LeakyReLU: introducen no linealidad en las redes.

**Recursos:**  
| 🎥 Videos | 📝 Artículos | 📘 Libros | 🧠 Ejercicios |
|----------|--------------|-----------|---------------|
| | [GeeksforGeeks – Sigmoid vs ReLU](https://www.geeksforgeeks.org/deep-learning/tanh-vs-sigmoid-vs-relu/) | *Deep Learning* (cap. 6) | [Ejercicio](ejercicio_funciones_activacion.md) |

---

## Redes Radiales (RBF)
**Descripción:**  
Redes que clasifican en función de la similitud con prototipos (funciones gaussianas).

**Recursos:**  
| 🎥 Videos | 📝 Artículos | 📘 Libros | 🧠 Ejercicios |
|----------|--------------|-----------|---------------|
| | [Chris McCormick – RBFN Tutorial](https://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/) | *Neural Networks and Learning Machines* | [Ejercicio](ejercicio_rbf.md) |

---

## Redes de Hopfield
**Descripción:**  
Modelos de memoria asociativa que almacenan patrones estables.

**Recursos:**  
| 🎥 Videos | 📝 Artículos | 📘 Libros | 🧠 Ejercicios |
|----------|--------------|-----------|---------------|
| [video](https://www.youtube.com/watch?v=vaqYWHlnhK8) | [GeeksforGeeks – Hopfield](https://www.geeksforgeeks.org/machine-learning/hopfield-neural-network/) | *Neural Networks and Learning Machines* | [Ejercicio](ejercicio_hopfield.md) |

---

## Regularización y optimización
**Descripción:**  
Técnicas como Dropout, BatchNorm y Early Stopping que mejoran la generalización.

**Recursos:**  
| 🎥 Videos | 📝 Artículos | 📘 Libros | 🧠 Ejercicios |
|----------|--------------|-----------|---------------|
| [video](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc) | [MachineLearningMastery – Dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) | *Deep Learning* (cap. 7) | [Ejercicio](ejercicio_regularizacion.md) |

---

## Redes convolucionales (CNN)
**Descripción:**  
Modelos especializados en imágenes usando convoluciones y pooling.

**Recursos:**  
| 🎥 Videos | 📝 Artículos | 📘 Libros | 🧠 Ejercicios |
|----------|--------------|-----------|---------------|
| [video](https://www.youtube.com/watch?v=iaSUYvmCekI) | [CS231n Notes](https://cs231n.github.io/convolutional-networks/) | *Deep Learning* (cap. 9) | [Ejercicio](ejercicio_cnn.md) |

---

## Redes recurrentes (RNN)
**Descripción:**  
Modelos para secuencias, con problemas de gradiente desvanecido.

**Recursos:**  
| 🎥 Videos | 📝 Artículos | 📘 Libros | 🧠 Ejercicios |
|----------|--------------|-----------|---------------|
| [video](https://www.youtube.com/watch?v=UNmqTiOnRfg) | [CS230 RNN Cheat Sheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks) | *Deep Learning* (cap. 10) | [Ejercicio](ejercicio_rnn.md) |

---

## LSTM y GRU
**Descripción:**  
Variantes de RNN que incorporan compuertas para manejar dependencias largas.

**Recursos:**  
| 🎥 Videos | 📝 Artículos | 📘 Libros | 🧠 Ejercicios |
|----------|--------------|-----------|---------------|
| [video](https://www.youtube.com/watch?v=8HyCNIVRbSU) | [GeeksforGeeks – LSTM vs GRU](https://www.geeksforgeeks.org/deep-learning/rnn-vs-lstm-vs-gru-vs-transformers/) | *Deep Learning* | [Ejercicio](ejercicio_lstm_gru.md) |

---

## Seq2Seq
**Descripción:**  
Modelos encoder-decoder para traducción, resumen y captioning.

**Recursos:**  
| 🎥 Videos | 📝 Artículos | 📘 Libros | 🧠 Ejercicios |
|----------|--------------|-----------|---------------|
| [video (conferencia)](https://www.youtube.com/watch?v=6D4EWKJgNn0) | [GeeksforGeeks – Seq2Seq](https://www.geeksforgeeks.org/machine-learning/seq2seq-model-in-machine-learning/) | *Deep Learning* | [Ejercicio](ejercicio_seq2seq.md) |

---

## Autoencoders y modelos generativos
**Descripción:**  
Modelos de codificación-decodificación (autoencoders) y generativos (GAN).

**Recursos:**  
| 🎥 Videos | 📝 Artículos | 📘 Libros | 🧠 Ejercicios |
|----------|--------------|-----------|---------------|
| | [GeeksforGeeks – Autoencoders](https://www.geeksforgeeks.org/machine-learning/auto-encoders/) | *Deep Learning* (cap. 14) | [Ejercicio](ejercicio_autoencoders.md) |

---

## Mecanismos de atención
**Descripción:**  
Self-attention asigna pesos según relevancia, clave en NLP moderno.

**Recursos:**  
| 🎥 Videos | 📝 Artículos | 📘 Libros | 🧠 Ejercicios |
|----------|--------------|-----------|---------------|
| [video](https://www.youtube.com/watch?v=SysgYptB198) | [DataCamp – Attention](https://www.datacamp.com/blog/attention-mechanism-in-llms-intuition) | *Transformers for NLP* (Rothman) | [Ejercicio](ejercicio_atencion.md) |

---

## Transformers
**Descripción:**  
Arquitectura basada en self-attention multi-cabeza y encoder-decoder.

**Recursos:**  
| 🎥 Videos | 📝 Artículos | 📘 Libros | 🧠 Ejercicios |
|----------|--------------|-----------|---------------|
| [video](https://www.youtube.com/watch?v=4Bdc55j80l8) | [D2L – Transformers](https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html) | [Attention is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762) | [Ejercicio](ejercicio_transformers.md) |

[Back to top](#deep-learning)

---

[Regresar al menú principal](../README.md)
