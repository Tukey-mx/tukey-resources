# Ejercicio: Mecanismos de Atenci贸n

##  Objetivo
Implementar un mecanismo de atenci贸n simple.

---

## З Instrucciones
1. Genera secuencias de longitud 10 con valores aleatorios.
2. Implementa self-attention:

Attention(Q,K,V) = softmax(QK岬 / d) V

3. Visualiza los pesos de atenci贸n.

---

##  Preguntas
- 驴Por qu茅 la atenci贸n mejora el contexto en NLP?
- 驴Qu茅 sucede si aumentamos el n煤mero de cabezas?

---
[Regresar a la p谩gina anterior](./DeepLearning.md)

[Regresar al men煤 principal](../README.md)
