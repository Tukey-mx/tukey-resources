# Ejercicio: Mecanismos de Atención

## 🎯 Objetivo
Implementar un mecanismo de atención simple.

---

## 🧩 Instrucciones
1. Genera secuencias de longitud 10 con valores aleatorios.
2. Implementa self-attention:

Attention(Q,K,V) = softmax(QKᵀ / √d) V

3. Visualiza los pesos de atención.

---

## 🧠 Preguntas
- ¿Por qué la atención mejora el contexto en NLP?
- ¿Qué sucede si aumentamos el número de cabezas?

---
[Regresar a la página anterior](./DeepLearning.md)

[Regresar al menú principal](../README.md)
